{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Code Generation Demo \n","\n","## Description\n","\n","This notebook demonstrates how to create a simple code generation interface using CodeLlama models from Hugging Face's `transformers` library and `gradio` for the user interface.\n","\n","### Main Features:\n","- Select from a list of pre-trained CodeLlama models.\n","- Input a description of the code you want, and receive generated code based on the prompt.\n","- Cache models to avoid reloading them multiple times.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Log in to Hugging Face Hub\n","\n","In this cell, we import the `login` function from the Hugging Face Hub and call it to authenticate with your Hugging Face account. This step is required to access private models, datasets, and other resources hosted on Hugging Face.\n","\n","### Directions to Generate Access Token:\n","1. Go to the [Hugging Face website](https://huggingface.co/).\n","2. Log in to your Hugging Face account.\n","3. Navigate to your **profile icon** on the top right, and click **Settings**.\n","4. Under **Access Tokens** (on the left sidebar), click **New Token** to generate a new access token.\n","5. Copy the generated token.\n","\n","### Usage:\n","When you run this cell, you'll be prompted to paste the access token, which grants access to your Hugging Face resources.\n","\n","> **Do not share your Access Tokens with anyone**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from huggingface_hub import login\n","login()"]},{"cell_type":"markdown","metadata":{},"source":["## Import Required Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gradio as gr\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{},"source":["## Basic Usage\n","\n","Below there are a list of available CodeLlama models to choose from. The function, `load_model`, loads the selected CodeLlama model along with its tokenizer. It uses Hugging Face's `AutoModelForCausalLM` and `AutoTokenizer` to load the pre-trained model and then returns a tuple containing the loaded model and tokenizer. Note that, you need to request access for CodeLlama models to use them. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["code_llama_models = {\n","    \"CodeLlama 7B Instruct\": \"meta-llama/CodeLlama-7b-Instruct-hf\",\n","    \"CodeLlama 13B Instruct\": \"meta-llama/CodeLlama-13b-Instruct-hf\",\n","    \"CodeLlama 34B Instruct\": \"meta-llama/CodeLlama-34b-Instruct-hf\",\n","    \"CodeLlama 70B Instruct\": \"meta-llama/CodeLlama-70b-Instruct-hf\",\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_model(model_name):\n","    \"\"\"Load the specified CodeLlama model.\"\"\"\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')\n","    model.to(device)\n","    return model, tokenizer"]},{"cell_type":"markdown","metadata":{},"source":["Cache models to avoid reloading"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_cache = {}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def generate_code(prompt, model_choice):\n","    \"\"\"Generate code using the selected CodeLlama model.\"\"\"\n","    if model_choice not in model_cache:\n","        model_cache[model_choice] = load_model(code_llama_models[model_choice])\n","    model, tokenizer = model_cache[model_choice]\n","    full_prompt = f\"Write a program based on the following description:\\n\\n\\\"{prompt}\\\"\\n\\nCode:\\n\"\n","    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(device)\n","    outputs = model.generate(\n","        **inputs,\n","        max_length=512,\n","        do_sample=True,\n","        temperature=0.5,\n","        top_p=0.9,\n","        num_return_sequences=1,\n","        eos_token_id=tokenizer.eos_token_id,\n","    )\n","    generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    code = generated_code.split(\"Code:\")[-1].strip()\n","    return code"]},{"cell_type":"markdown","metadata":{},"source":["The cell below defines a simple Gradio interface. The interface consists of a dropdown menu for users to select specific CodeLlama model, a code output block is used to display the generated code, specifically formatted for Python, a textbox to enter queries and a window that displays the conversation between the user and the assistant."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with gr.Blocks() as demo:\n","    gr.Markdown(\"<h1><center>Code Generation with CodeLlama Models</center></h1>\")\n","    model_choice = gr.Dropdown(list(code_llama_models.keys()), label=\"Select CodeLlama Model\")\n","    prompt_input = gr.Textbox(label=\"Describe the code you want\", lines=2)\n","    code_output = gr.Code(language=\"python\", label=\"Generated Code\")\n","    generate_button = gr.Button(\"Generate Code\")\n","    generate_button.click(generate_code, [prompt_input, model_choice], code_output)"]},{"cell_type":"markdown","metadata":{},"source":["Launch the interface"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["demo.launch()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":2}
