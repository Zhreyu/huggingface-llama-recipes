{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import gradio as gr\n", "import torch\n", "from transformers import AutoModelForCausalLM, AutoTokenizer"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["code_llama_models = {\n", "    \"CodeLlama 7B Instruct\": \"meta-llama/CodeLlama-7b-Instruct-hf\",\n", "    \"CodeLlama 13B Instruct\": \"meta-llama/CodeLlama-13b-Instruct-hf\",\n", "    \"CodeLlama 34B Instruct\": \"meta-llama/CodeLlama-34b-Instruct-hf\",\n", "    \"CodeLlama 70B Instruct\": \"meta-llama/CodeLlama-70b-Instruct-hf\",\n", "}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_model(model_name):\n", "    \"\"\"Load the specified CodeLlama model.\"\"\"\n", "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n", "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')\n", "    model.to(device)\n", "    return model, tokenizer"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Cache models"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model_cache = {}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def generate_code(prompt, model_choice):\n", "    \"\"\"Generate code using the selected CodeLlama model.\"\"\"\n", "    if model_choice not in model_cache:\n", "        model_cache[model_choice] = load_model(code_llama_models[model_choice])\n", "    model, tokenizer = model_cache[model_choice]\n", "    full_prompt = f\"Write a program based on the following description:\\n\\n\\\"{prompt}\\\"\\n\\nCode:\\n\"\n", "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(device)\n", "    outputs = model.generate(\n", "        **inputs,\n", "        max_length=512,\n", "        do_sample=True,\n", "        temperature=0.5,\n", "        top_p=0.9,\n", "        num_return_sequences=1,\n", "        eos_token_id=tokenizer.eos_token_id,\n", "    )\n", "    generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n", "    code = generated_code.split(\"Code:\")[-1].strip()\n", "    return code"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Gradio interface"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with gr.Blocks() as demo:\n", "    gr.Markdown(\"<h1><center>Code Generation with CodeLlama Models</center></h1>\")\n", "    model_choice = gr.Dropdown(list(code_llama_models.keys()), label=\"Select CodeLlama Model\")\n", "    prompt_input = gr.Textbox(label=\"Describe the code you want\", lines=2)\n", "    code_output = gr.Code(language=\"python\", label=\"Generated Code\")\n", "    generate_button = gr.Button(\"Generate Code\")\n", "    generate_button.click(generate_code, [prompt_input, model_choice], code_output)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["demo.launch()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}