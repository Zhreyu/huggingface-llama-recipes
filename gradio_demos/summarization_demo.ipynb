{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Text Summarization Demo\n","\n","## Description\n","\n","This notebook provides an interactive demo for text summarization using pre-trained Llama models from Hugging Face's `transformers` library, integrated with `gradio` for a simple and user-friendly interface.\n","\n","### Main Features:\n","- Select from a variety of pre-trained Llama models for text summarization.\n","- Input any text and generate a concise summary.\n","- Display the summary in a dedicated output textbox.\n","- Cache models to avoid reloading and improve performance."]},{"cell_type":"markdown","metadata":{},"source":["## Log in to Hugging Face Hub\n","\n","In this cell, we import the `login` function from the Hugging Face Hub and call it to authenticate with your Hugging Face account. This step is required to access private models, datasets, and other resources hosted on Hugging Face.\n","\n","### Directions to Generate Access Token:\n","1. Go to the [Hugging Face website](https://huggingface.co/).\n","2. Log in to your Hugging Face account.\n","3. Navigate to your **profile icon** on the top right, and click **Settings**.\n","4. Under **Access Tokens** (on the left sidebar), click **New Token** to generate a new access token.\n","5. Copy the generated token.\n","\n","### Usage:\n","When you run this cell, you'll be prompted to paste the access token, which grants access to your Hugging Face resources.\n","\n","> **Do not share your Access Tokens with anyone**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from huggingface_hub import login\n","login()"]},{"cell_type":"markdown","metadata":{},"source":["## Import Required Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gradio as gr\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{},"source":["## Basic Usage\n","\n","Below there are a list of available Llama models to choose from. The function, `load_model`, loads the selected Llama model along with its tokenizer. It uses Hugging Face's `AutoModelForCausalLM` and `AutoTokenizer` to load the pre-trained model and then returns a tuple containing the loaded model and tokenizer. Note that, for each version of Llama you need to seperately request access."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["llama_models = {\n","    \"Llama 3 70B Instruct\": \"meta-llama/Meta-Llama-3-70B-Instruct\",\n","    \"Llama 3 8B Instruct\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n","    \"Llama 3.1 70B Instruct\": \"meta-llama/Llama-3.1-70B-Instruct\",\n","    \"Llama 3.1 8B Instruct\": \"meta-llama/Llama-3.1-8B-Instruct\",\n","    \"Llama 3.2 3B Instruct\": \"meta-llama/Llama-3.2-3B-Instruct\",\n","    \"Llama 3.2 1B Instruct\": \"meta-llama/Llama-3.2-1B-Instruct\",\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_model(model_name):\n","    \"\"\"Load the specified Llama model.\"\"\"\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto')\n","    model.to(device)\n","    return model, tokenizer"]},{"cell_type":"markdown","metadata":{},"source":["Cache models to avoid reloading"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_cache = {}"]},{"cell_type":"markdown","metadata":{},"source":["The `summarize_text` function generates a summary of the input text using the selected Llama model. It checks if the model is cached, loads it if necessary, and then formats the input text as a prompt for summarization. The model processes the input, and the function returns the generated summary by extracting it from the model's output."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def summarize_text(text, model_choice):\n","    \"\"\"Summarize text using the selected Llama model.\"\"\"\n","    if model_choice not in model_cache:\n","        model_cache[model_choice] = load_model(llama_models[model_choice])\n","    model, tokenizer = model_cache[model_choice]\n","    prompt = f\"Summarize the following text:\\n\\n\\\"{text}\\\"\\n\\nSummary:\"\n","    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n","    outputs = model.generate(**inputs, max_length=512, do_sample=False)\n","    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    summary = generated_text.split(\"Summary:\")[-1].strip()\n","    return summary"]},{"cell_type":"markdown","metadata":{},"source":["The cell below defines a simple Gradio interface for text summarization. The interface consists of a dropdown menu where users can select a specific Llama model for summarization, a textbox where users can input the text they want to summarize, and an output box to display the generated summary. Once the \"Summarize\" button is clicked, the selected model processes the input and returns a concise summary of the provided text."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with gr.Blocks() as demo:\n","    gr.Markdown(\"<h1><center>Summarization with Llama Models</center></h1>\")\n","    model_choice = gr.Dropdown(list(llama_models.keys()), label=\"Select Llama Model\")\n","    input_text = gr.Textbox(label=\"Enter text to summarize\", lines=10)\n","    output_text = gr.Textbox(label=\"Summary\")\n","    summarize_button = gr.Button(\"Summarize\")\n","    summarize_button.click(summarize_text, [input_text, model_choice], output_text)"]},{"cell_type":"markdown","metadata":{},"source":["Launch the interface"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["demo.launch()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":2}
