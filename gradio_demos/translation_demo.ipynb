{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Translation Demo\n","\n","## Description\n","\n","This notebook provides an interactive demo for text translation using pre-trained Llama models from Hugging Face's `transformers` library, integrated with `gradio` for a user-friendly interface. Users can select from multiple languages to translate text from English into the desired target language.\n","\n","### Main Features:\n","- Select from a variety of pre-trained Llama models for text translation.\n","- Input text in English and translate it into one of 20 supported target languages, including French, Spanish, German, Chinese, and more.\n","- Display the translated text in a dedicated output textbox.\n","- Cache models to avoid reloading and improve performance.\n","- Select the appropriate translation pipeline based on the chosen target language.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Log in to Hugging Face Hub\n","\n","In this cell, we import the `login` function from the Hugging Face Hub and call it to authenticate with your Hugging Face account. This step is required to access private models, datasets, and other resources hosted on Hugging Face.\n","\n","### Directions to Generate Access Token:\n","1. Go to the [Hugging Face website](https://huggingface.co/).\n","2. Log in to your Hugging Face account.\n","3. Navigate to your **profile icon** on the top right, and click **Settings**.\n","4. Under **Access Tokens** (on the left sidebar), click **New Token** to generate a new access token.\n","5. Copy the generated token.\n","\n","### Usage:\n","When you run this cell, you'll be prompted to paste the access token, which grants access to your Hugging Face resources.\n","\n","> **Do not share your Access Tokens with anyone**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from huggingface_hub import login\n","login()"]},{"cell_type":"markdown","metadata":{},"source":["## Import required libraries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gradio as gr\n","import torch\n","from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = 0 if torch.cuda.is_available() else -1"]},{"cell_type":"markdown","metadata":{},"source":["## Basic Usage\n","\n","Below there is a list of available Llama models to choose from. The function, `load_translation_model`, loads the selected Llama model along with its tokenizer. It uses Hugging Face's `AutoModelForSeq2SeqLM` and `AutoTokenizer` to load the pre-trained model and then returns a translation pipeline. The user can select the target language for translation from a dropdown menu and translate the provided text into languages such as French, Spanish, German, and more.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["llama_models = {\n","    \"Llama 3 70B Instruct\": \"meta-llama/Meta-Llama-3-70B-Instruct\",\n","    \"Llama 3 8B Instruct\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n","    \"Llama 3.1 70B Instruct\": \"meta-llama/Llama-3.1-70B-Instruct\",\n","    \"Llama 3.1 8B Instruct\": \"meta-llama/Llama-3.1-8B-Instruct\",\n","    \"Llama 3.2 3B Instruct\": \"meta-llama/Llama-3.2-3B-Instruct\",\n","    \"Llama 3.2 1B Instruct\": \"meta-llama/Llama-3.2-1B-Instruct\",\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_translation_model(model_name):\n","    \"\"\"Load the specified Llama translation model.\"\"\"\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n","    translator = pipeline('translation_en_to_fr', model=model, tokenizer=tokenizer, device=device)\n","    return translator"]},{"cell_type":"markdown","metadata":{},"source":["Cache models to avoid reloading"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_cache = {}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def translate_text(text, model_choice, target_language):\n","    \"\"\"Translate text using the selected Llama model and target language.\"\"\"\n","    if model_choice not in model_cache:\n","        model_cache[model_choice] = load_translation_model(llama_models[model_choice])\n","    translator = model_cache[model_choice]\n","    \n","    # Map target language to the appropriate translation task\n","    language_map = {\n","        \"French\": \"translation_en_to_fr\",\n","        \"Spanish\": \"translation_en_to_es\",\n","        \"German\": \"translation_en_to_de\",\n","        \"Italian\": \"translation_en_to_it\",\n","        \"Portuguese\": \"translation_en_to_pt\",\n","        \"Dutch\": \"translation_en_to_nl\",\n","        \"Russian\": \"translation_en_to_ru\",\n","        \"Chinese\": \"translation_en_to_zh\",\n","        \"Japanese\": \"translation_en_to_ja\",\n","        \"Korean\": \"translation_en_to_ko\",\n","        \"Arabic\": \"translation_en_to_ar\",\n","        \"Hindi\": \"translation_en_to_hi\",\n","        \"Bengali\": \"translation_en_to_bn\",\n","        \"Greek\": \"translation_en_to_el\",\n","        \"Turkish\": \"translation_en_to_tr\",\n","        \"Swedish\": \"translation_en_to_sv\",\n","        \"Norwegian\": \"translation_en_to_no\",\n","        \"Danish\": \"translation_en_to_da\",\n","        \"Finnish\": \"translation_en_to_fi\",\n","        \"Polish\": \"translation_en_to_pl\"\n","    }\n","    \n","    translation_task = language_map.get(target_language, \"translation_en_to_fr\")\n","    translator = pipeline(translation_task, model=translator.model, tokenizer=translator.tokenizer, device=device)\n","    \n","    result = translator(text)[0]\n","    return result['translation_text']"]},{"cell_type":"markdown","metadata":{},"source":["The cell below defines a simple Gradio interface for text translation. It includes a dropdown menu where users can select a specific Llama model for translation, a textbox where users can input the English text to be translated, and another dropdown to choose the target language from a list of available languages. \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with gr.Blocks() as demo:\n","    gr.Markdown(\"<h1><center>Translation with Llama Models</center></h1>\")\n","    model_choice = gr.Dropdown(list(llama_models.keys()), label=\"Select Llama Model\")\n","    input_text = gr.Textbox(label=\"Enter text to translate\", lines=4)\n","    target_language = gr.Dropdown(\n","        [\"French\", \"Spanish\", \"German\", \"Italian\", \"Portuguese\", \"Dutch\", \"Russian\", \"Chinese\", \"Japanese\", \"Korean\", \"Arabic\", \"Hindi\", \"Bengali\", \"Greek\", \"Turkish\", \"Swedish\", \"Norwegian\", \"Danish\", \"Finnish\", \"Polish\"],\n","        label=\"Select Target Language\"\n","    )\n","    output_text = gr.Textbox(label=\"Translated Text\")\n","    gr.Button(\"Translate\").click(translate_text, [input_text, model_choice, target_language], output_text)"]},{"cell_type":"markdown","metadata":{},"source":["Launch the interface"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["demo.launch()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":2}
