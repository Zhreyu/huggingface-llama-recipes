{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import gradio as gr\n", "import torch\n", "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Check if CUDA is available"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["device = 0 if torch.cuda.is_available() else -1"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["llama_models = {\n", "    \"Meta-Llama 3 70B Instruct\": \"meta-llama/Meta-Llama-3-70B-Instruct\",\n", "    \"Meta-Llama 3 8B Instruct\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n", "    \"Llama 3.1 70B Instruct\": \"meta-llama/Llama-3.1-70B-Instruct\",\n", "    \"Llama 3.1 8B Instruct\": \"meta-llama/Llama-3.1-8B-Instruct\",\n", "}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def load_model(model_name):\n", "    \"\"\"Load the specified Llama model.\"\"\"\n", "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n", "    model = AutoModelForCausalLM.from_pretrained(model_name)\n", "    generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device)\n", "    return generator"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Cache models to avoid reloading"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model_cache = {}"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def chatbot_interface(user_input, history, model_choice):\n", "    \"\"\"Generate chatbot responses using the selected Llama model.\"\"\"\n", "    if model_choice not in model_cache:\n", "        model_cache[model_choice] = load_model(llama_models[model_choice])\n", "    generator = model_cache[model_choice]\n", "    \n", "    if history is None:\n", "        history = []\n", "    history.append((\"User\", user_input))\n", "    prompt = \"\\n\".join([f\"{speaker}: {text}\" for speaker, text in history]) + \"\\nAssistant:\"\n", "    \n", "    # Generate response\n", "    response = generator(prompt, max_length=512, pad_token_id=generator.tokenizer.eos_token_id,\n", "                         do_sample=True, temperature=0.7, top_p=0.9)[0]['generated_text']\n", "    assistant_reply = response.split(\"Assistant:\")[-1].strip()\n", "    history.append((\"Assistant\", assistant_reply))\n", "    \n", "    return history, history"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Gradio interface"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["with gr.Blocks() as demo:\n", "    gr.Markdown(\"<h1><center>Chat with Llama Models</center></h1>\")\n", "    model_choice = gr.Dropdown(list(llama_models.keys()), label=\"Select Llama Model\")\n", "    chatbot = gr.Chatbot()\n", "    state = gr.State([])\n", "    txt_input = gr.Textbox(show_label=False, placeholder=\"Type your message here...\")\n", "    def respond(user_input, history, model_choice):\n", "        return chatbot_interface(user_input, history, model_choice)\n", "    txt_input.submit(respond, [txt_input, state, model_choice], [chatbot, state])\n", "    gr.Button(\"Submit\").click(respond, [txt_input, state, model_choice], [chatbot, state])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["demo.launch()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}