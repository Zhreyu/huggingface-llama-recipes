{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Chatbot Demo \n","\n","## Description\n","\n","This notebook demonstrates how to create a simple chatbot interface using Llama models via Hugging Face's `transformers` library and `gradio` for the user interface. \n","\n","### Main Features:\n","- Select from a list of pre-trained Llama models.\n","- Input text and receive responses from the selected model.\n","- Cache models to avoid reloading them multiple times."]},{"cell_type":"markdown","metadata":{},"source":["## Log in to Hugging Face Hub\n","\n","In this cell, we import the `login` function from the Hugging Face Hub and call it to authenticate with your Hugging Face account. This step is required to access private models, datasets, and other resources hosted on Hugging Face.\n","\n","### Directions to Generate Access Token:\n","1. Go to the [Hugging Face website](https://huggingface.co/).\n","2. Log in to your Hugging Face account.\n","3. Navigate to your **profile icon** on the top right, and click **Settings**.\n","4. Under **Access Tokens** (on the left sidebar), click **New Token** to generate a new access token.\n","5. Copy the generated token.\n","\n","### Usage:\n","When you run this cell, you'll be prompted to paste the access token, which grants access to your Hugging Face resources.\n","\n","> **Do not share your Access Tokens with anyone**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from huggingface_hub import login\n","login()"]},{"cell_type":"markdown","metadata":{},"source":["## Import Required Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gradio as gr\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = 0 if torch.cuda.is_available() else -1"]},{"cell_type":"markdown","metadata":{},"source":["## Basic Usage\n","\n","Below there are a list of available Llama models to choose from. The function, `load_model`, loads the selected Llama model along with its tokenizer. It uses Hugging Face's `AutoModelForCausalLM` and `AutoTokenizer` to load the pre-trained model and return a text generation pipeline. Note that, for each version of Llama you need to seperately request access to be able to use them."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["llama_models = {\n","    \"Llama 3 70B Instruct\": \"meta-llama/Meta-Llama-3-70B-Instruct\",\n","    \"Llama 3 8B Instruct\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n","    \"Llama 3.1 70B Instruct\": \"meta-llama/Llama-3.1-70B-Instruct\",\n","    \"Llama 3.1 8B Instruct\": \"meta-llama/Llama-3.1-8B-Instruct\",\n","    \"Llama 3.2 3B Instruct\": \"meta-llama/Llama-3.2-3B-Instruct\",\n","    \"Llama 3.2 1B Instruct\": \"meta-llama/Llama-3.2-1B-Instruct\",\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_model(model_name):\n","    \"\"\"Load the specified Llama model.\"\"\"\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model = AutoModelForCausalLM.from_pretrained(model_name)\n","    generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device)\n","    return generator"]},{"cell_type":"markdown","metadata":{},"source":["Cache models to avoid reloading"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_cache = {}"]},{"cell_type":"markdown","metadata":{},"source":["This cell defines a `system_prompts` dictionary that provides specific instructions for different chatbot tasks:\n","\n","- **Conversation**: A general-purpose helpful assistant.\n","- **Sentiment analysis**: An assistant that analyzes user input and assigns a sentiment label along with a brief explanation behind the reasoning.\n","- **Summarization**: An assistant that summarizes user input, focusing on key points and main ideas.\n","\n","Each prompt includes a role (\"system\") and task-specific content to guide the chatbot's behavior during interactions."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["system_prompts = {\"conversation\": {\n","                        \"role\" : \"system\", \n","                        \"content\" : \"You are helpful assistant\"\n","                  },\n","                  \"sentiment analysis\": {\n","                        \"role\": \"system\",\n","                        \"content\": \"You are a helpful assistant specialized in sentiment analysis. Analyze the user's input and provide a sentiment label (positive, negative, or neutral) along with a brief explanation of your reasoning.\"\n","                  },\n","                  \"summarization\": {\n","                        \"role\": \"system\",\n","                        \"content\": \"You are a helpful assistant specialized in summarizing text. Provide concise and coherent summaries by focusing on the key points and main ideas in the user input.\"\n","                  }}"]},{"cell_type":"markdown","metadata":{},"source":["The `generate_chat` function generates chatbot responses using a selected Llama model and task. It first checks if the model is cached, loading it if necessary. The appropriate system prompt is chosen based on the `task_choice` (e.g., \"conversation\", \"summarization\", or \"sentiment_analysis\"). The function builds the conversation context from the prompt and chat history, then adds the user's input. This context is passed to the model for response generation. The response is extracted, appended to the history, and the updated conversation history is returned."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def generate_chat(user_input, history, model_choice, task_choice):\n","    \"\"\"Generate chatbot responses using the selected Llama model and task.\"\"\"\n","    if model_choice not in model_cache:\n","        model_cache[model_choice] = load_model(llama_models[model_choice])\n","    generator = model_cache[model_choice]\n","\n","    system_prompt = system_prompts.get(task_choice.lower(), system_prompts[\"conversation\"])\n","\n","    messages = [system_prompt]\n","\n","    if history is None:\n","        history = []\n","\n","    for message in history:\n","        messages.append({\"role\": \"user\", \"content\": str(message[0])})\n","        messages.append({\"role\": \"assistant\", \"content\": str(message[1])})\n","\n","    messages.append({\"role\": \"user\", \"content\": str(user_input)})\n","\n","    response = generator(\n","        messages,\n","        max_length=512,\n","        pad_token_id=generator.tokenizer.eos_token_id,\n","        do_sample=True,\n","        temperature=0.7,\n","        top_p=0.9\n","    )[-1][\"generated_text\"][-1][\"content\"]\n","\n","    history.append((user_input, response))\n","\n","    return history"]},{"cell_type":"markdown","metadata":{},"source":["The cell below defines a simple Gradio interface. The interface consists of two dropdown menus, one for users to select specific Llama model and the other to select the task, a textbox to enter queries and a window that displays the conversation between the user and the assistant."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with gr.Blocks() as demo:\n","    gr.Markdown(\"<h1><center>Chat with Llama Models</center></h1>\")\n","    model_choice = gr.Dropdown(list(llama_models.keys()), label=\"Select Llama Model\")\n","\n","    task_choice = gr.Dropdown([\"Conversation\", \"Summarization\", \"Sentiment Analysis\"], label=\"Select Task\")\n","\n","    chatbot = gr.Chatbot()\n","    txt_input = gr.Textbox(show_label=False, placeholder=\"Type your message here...\")\n","\n","    def respond(user_input, chat_history, model_choice, task_choice):\n","        updated_history = generate_chat(user_input, chat_history, model_choice, task_choice)\n","        return \"\", updated_history\n","\n","    txt_input.submit(respond, [txt_input, chatbot, model_choice, task_choice], [txt_input, chatbot])\n","\n","    submit_btn = gr.Button(\"Submit\")\n","    submit_btn.click(respond, [txt_input, chatbot, model_choice, task_choice], [txt_input, chatbot])"]},{"cell_type":"markdown","metadata":{},"source":["Launch the interface"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["demo.launch()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":2}
