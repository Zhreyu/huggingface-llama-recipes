{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chatbot Demo \n",
        "\n",
        "## Description\n",
        "\n",
        "This notebook demonstrates how to create a simple chatbot interface using Llama models via Hugging Face's `transformers` library and `gradio` for the user interface. \n",
        "\n",
        "### Main Features:\n",
        "- Select from a list of pre-trained Llama models.\n",
        "- Input text and receive responses from the selected model.\n",
        "- Cache models to avoid reloading them multiple times."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Log in to Hugging Face Hub\n",
        "\n",
        "Log in to your Hugging Face account to access the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = 0 if torch.cuda.is_available() else -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Selection\n",
        "\n",
        "Below is a list of available Llama models to choose from. The `load_model` function loads the selected Llama model along with its tokenizer. It uses Hugging Face's `AutoModelForCausalLM` and `AutoTokenizer` to load the pre-trained model and return a text generation pipeline. Note that for each version of Llama, you need to separately request access to be able to use them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llama_models = {\n",
        "    \"Llama 3 70B Instruct\": \"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
        "    \"Llama 3 8B Instruct\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "    \"Llama 3.1 70B Instruct\": \"meta-llama/Llama-3.1-70B-Instruct\",\n",
        "    \"Llama 3.1 8B Instruct\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "    \"Llama 3.2 3B Instruct\": \"meta-llama/Llama-3.2-3B-Instruct\",\n",
        "    \"Llama 3.2 1B Instruct\": \"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model(model_name):\n",
        "    \"\"\"Load the specified Llama model.\"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "    generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device)\n",
        "    return generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Caching\n",
        "\n",
        "Cache models to avoid reloading them multiple times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_cache = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chat Generation Function\n",
        "\n",
        "The `generate_chat` function generates chatbot responses using the selected Llama model. It checks if the model is cached and loads it if necessary. The function builds the conversation context from the chat history and adds the user's input. This context is passed to the model for response generation. The response is extracted, appended to the history, and the updated conversation history is returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_chat(user_input, history, model_choice):\n",
        "    \"\"\"Generate chatbot responses using the selected Llama model.\"\"\"\n",
        "    if model_choice not in model_cache:\n",
        "        model_cache[model_choice] = load_model(llama_models[model_choice])\n",
        "    generator = model_cache[model_choice]\n",
        "\n",
        "    if history is None:\n",
        "        history = []\n",
        "\n",
        "    # Build the conversation history\n",
        "    conversation = ''\n",
        "    for user_msg, assistant_msg in history:\n",
        "        conversation += \"User: \" + user_msg + \"\\n\"\n",
        "        conversation += \"Assistant: \" + assistant_msg + \"\\n\"\n",
        "\n",
        "    # Add the latest user input\n",
        "    conversation += \"User: \" + user_input + \"\\n\"\n",
        "    conversation += \"Assistant:\"\n",
        "\n",
        "    # Generate the assistant's response\n",
        "    output = generator(\n",
        "        conversation,\n",
        "        max_length=512,\n",
        "        pad_token_id=generator.tokenizer.eos_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9\n",
        "    )[0][\"generated_text\"]\n",
        "\n",
        "    # Extract the assistant's response\n",
        "    assistant_response = output[len(conversation):].strip()\n",
        "\n",
        "    history.append((user_input, assistant_response))\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradio Interface\n",
        "\n",
        "The cell below defines a simple Gradio interface. The interface consists of a dropdown menu for users to select a specific Llama model, a textbox to enter queries, and a window that displays the conversation between the user and the assistant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"<h1><center>Chat with Llama Models</center></h1>\")\n",
        "\n",
        "    model_choice = gr.Dropdown(list(llama_models.keys()), label=\"Select Llama Model\")\n",
        "\n",
        "    chatbot = gr.Chatbot(label=\"Chatbot Interface\")\n",
        "    txt_input = gr.Textbox(show_label=False, placeholder=\"Type your message here...\")\n",
        "\n",
        "    def respond(user_input, chat_history, model_choice):\n",
        "        if model_choice is None:\n",
        "            model_choice = list(llama_models.keys())[0]  \n",
        "        updated_history = generate_chat(user_input, chat_history, model_choice)\n",
        "        return \"\", updated_history\n",
        "\n",
        "    txt_input.submit(respond, [txt_input, chatbot, model_choice], [txt_input, chatbot])\n",
        "\n",
        "    submit_btn = gr.Button(\"Submit\")\n",
        "    submit_btn.click(respond, [txt_input, chatbot, model_choice], [txt_input, chatbot])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Launch the Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "demo.launch()\n"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
