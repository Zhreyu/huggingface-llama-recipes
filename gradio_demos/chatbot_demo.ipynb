{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Chatbot Demo \n","\n","## Description\n","\n","This notebook demonstrates how to create a simple chatbot interface using Llama models via Hugging Face's `transformers` library and `gradio` for the user interface. \n","\n","### Main Features:\n","- Select from a list of pre-trained Llama models.\n","- Input text and receive responses from the selected model.\n","- Cache models to avoid reloading them multiple times."]},{"cell_type":"markdown","metadata":{},"source":["## Log in to Hugging Face Hub\n","\n","In this cell, we import the `login` function from the Hugging Face Hub and call it to authenticate with your Hugging Face account. This step is required to access private models, datasets, and other resources hosted on Hugging Face.\n","\n","### Directions to Generate Access Token:\n","1. Go to the [Hugging Face website](https://huggingface.co/).\n","2. Log in to your Hugging Face account.\n","3. Navigate to your **profile icon** on the top right, and click **Settings**.\n","4. Under **Access Tokens** (on the left sidebar), click **New Token** to generate a new access token.\n","5. Copy the generated token.\n","\n","### Usage:\n","When you run this cell, you'll be prompted to paste the access token, which grants access to your Hugging Face resources.\n","\n","> **Do not share your Access Tokens with anyone**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from huggingface_hub import login\n","login()"]},{"cell_type":"markdown","metadata":{},"source":["## Import Required Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gradio as gr\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = 0 if torch.cuda.is_available() else -1"]},{"cell_type":"markdown","metadata":{},"source":["## Basic Usage\n","\n","Below there are a list of available Llama models to choose from. The function, `load_model`, loads the selected Llama model along with its tokenizer. It uses Hugging Face's `AutoModelForCausalLM` and `AutoTokenizer` to load the pre-trained model and return a text generation pipeline. Note that, for each version of Llama you need to seperately request access to be able to use them."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["llama_models = {\n","    \"Llama 3 70B Instruct\": \"meta-llama/Meta-Llama-3-70B-Instruct\",\n","    \"Llama 3 8B Instruct\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n","    \"Llama 3.1 70B Instruct\": \"meta-llama/Llama-3.1-70B-Instruct\",\n","    \"Llama 3.1 8B Instruct\": \"meta-llama/Llama-3.1-8B-Instruct\",\n","    \"Llama 3.2 3B Instruct\": \"meta-llama/Llama-3.2-3B-Instruct\",\n","    \"Llama 3.2 1B Instruct\": \"meta-llama/Llama-3.2-1B-Instruct\",\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_model(model_name):\n","    \"\"\"Load the specified Llama model.\"\"\"\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model = AutoModelForCausalLM.from_pretrained(model_name)\n","    generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device)\n","    return generator"]},{"cell_type":"markdown","metadata":{},"source":["Cache models to avoid reloading"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_cache = {}"]},{"cell_type":"markdown","metadata":{},"source":["The function, `chatbot_interface`, handles generating responses from the Llama models. It loads the model if it is not already cached. Builds the conversation context using the chat history then calls the model's text generation pipeline to generate a response based on the user's input. Finally, it extracts the response from the model and appends it to the conversation history."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def chatbot_interface(user_input, history, model_choice):\n","    \"\"\"Generate chatbot responses using the selected Llama model.\"\"\"\n","    if model_choice not in model_cache:\n","        model_cache[model_choice] = load_model(llama_models[model_choice])\n","    generator = model_cache[model_choice]\n","    \n","    if history is None:\n","        history = []\n","    \n","    conversation = \"\"\n","    for user_msg, assistant_msg in history:\n","        conversation += f\"User: {user_msg}\\nAssistant: {assistant_msg}\\n\"\n","    conversation += f\"User: {user_input}\\nAssistant:\"\n","\n","    response = generator(\n","        conversation,\n","        max_length=512,\n","        pad_token_id=generator.tokenizer.eos_token_id,\n","        do_sample=True,\n","        temperature=0.7,\n","        top_p=0.9\n","    )[0]['generated_text']\n","    \n","    assistant_reply = response.split(\"Assistant:\")[-1].strip()\n","    \n","    history.append((user_input, assistant_reply))\n","    \n","    return history"]},{"cell_type":"markdown","metadata":{},"source":["The cell below defines a simple Gradio interface. The interface consists of a dropdown menu for users to select specific Llama model, a textbox to enter queries and a window that displays the conversation between the user and the assistant."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with gr.Blocks() as demo:\n","    gr.Markdown(\"<h1><center>Chat with Llama Models</center></h1>\")\n","    model_choice = gr.Dropdown(list(llama_models.keys()), label=\"Select Llama Model\")\n","    chatbot = gr.Chatbot()\n","    txt_input = gr.Textbox(show_label=False, placeholder=\"Type your message here...\")\n","\n","    def respond(user_input, chat_history, model_choice):\n","        updated_history = chatbot_interface(user_input, chat_history, model_choice)\n","        return \"\", updated_history  \n","    \n","    txt_input.submit(respond, [txt_input, chatbot, model_choice], [txt_input, chatbot])\n","    submit_btn = gr.Button(\"Submit\")\n","    submit_btn.click(respond, [txt_input, chatbot, model_choice], [txt_input, chatbot])"]},{"cell_type":"markdown","metadata":{},"source":["Launch the interface"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["demo.launch()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":2}
