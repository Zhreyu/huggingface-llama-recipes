{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Sentiment Analysis Demo\n","\n","## Description\n","\n","This notebook provides an interactive demo for performing sentiment analysis using pre-trained Llama models from Hugging Face's `transformers` library, integrated with `gradio` for a user-friendly interface.\n","\n","### Main Features:\n","- Select from a variety of pre-trained Llama models for sentiment analysis.\n","- Input any text and analyze its sentiment (e.g., positive, negative, neutral).\n","- Display the sentiment result along with the confidence score.\n","- Cache models to avoid reloading."]},{"cell_type":"markdown","metadata":{},"source":["## Log in to Hugging Face Hub\n","\n","In this cell, we import the `login` function from the Hugging Face Hub and call it to authenticate with your Hugging Face account. This step is required to access private models, datasets, and other resources hosted on Hugging Face.\n","\n","### Directions to Generate Access Token:\n","1. Go to the [Hugging Face website](https://huggingface.co/).\n","2. Log in to your Hugging Face account.\n","3. Navigate to your **profile icon** on the top right, and click **Settings**.\n","4. Under **Access Tokens** (on the left sidebar), click **New Token** to generate a new access token.\n","5. Copy the generated token.\n","\n","### Usage:\n","When you run this cell, you'll be prompted to paste the access token, which grants access to your Hugging Face resources.\n","\n","> **Do not share your Access Tokens with anyone**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from huggingface_hub import login\n","login()"]},{"cell_type":"markdown","metadata":{},"source":["## Import Required Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import gradio as gr\n","import torch\n","from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = 0 if torch.cuda.is_available() else -1"]},{"cell_type":"markdown","metadata":{},"source":["## Basic Usage\n","\n","Below is a list of available Llama models for sentiment analysis. The function `load_sentiment_model` loads the selected model and its tokenizer using Hugging Face's `AutoModelForSequenceClassification` and `AutoTokenizer`. It returns a sentiment analysis pipeline that can be used to analyze the sentiment of the provided text. Note that, for each version of Llama you need to seperately request access to be able to use them."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["llama_models = {\n","    \"Llama 3 70B Instruct\": \"meta-llama/Meta-Llama-3-70B-Instruct\",\n","    \"Llama 3 8B Instruct\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n","    \"Llama 3.1 70B Instruct\": \"meta-llama/Llama-3.1-70B-Instruct\",\n","    \"Llama 3.1 8B Instruct\": \"meta-llama/Llama-3.1-8B-Instruct\",\n","    \"Llama 3.2 3B Instruct\": \"meta-llama/Llama-3.2-3B-Instruct\",\n","    \"Llama 3.2 1B Instruct\": \"meta-llama/Llama-3.2-1B-Instruct\",\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_sentiment_model(model_name):\n","    \"\"\"Load the specified Llama sentiment model.\"\"\"\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","    sentiment_analyzer = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer, device=device)\n","    return sentiment_analyzer"]},{"cell_type":"markdown","metadata":{},"source":["Cache models to avoid reloading"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_cache = {}"]},{"cell_type":"markdown","metadata":{},"source":["The `analyze_sentiment` function performs sentiment analysis using the selected Llama model. It first checks if the model is cached and loads it if not. The function then uses the cached model to analyze the input text, returning the sentiment label (e.g., \"positive\" or \"negative\") along with the confidence score."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def analyze_sentiment(text, model_choice):\n","    \"\"\"Perform sentiment analysis using the selected Llama model.\"\"\"\n","    if model_choice not in model_cache:\n","        model_cache[model_choice] = load_sentiment_model(llama_models[model_choice])\n","    sentiment_analyzer = model_cache[model_choice]\n","    result = sentiment_analyzer(text)[0]\n","    return f\"Sentiment: {result['label']} (Confidence: {result['score']:.2f})\""]},{"cell_type":"markdown","metadata":{},"source":["The cell below defines a simple Gradio interface. The interface consists of a dropdown menu where users can select a specific Llama model for sentiment analysis, a textbox where users can input text for sentiment evaluation, and an output box to display the sentiment result, including the label (e.g., positive or negative) and the confidence score."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with gr.Blocks() as demo:\n","    gr.Markdown(\"<h1><center>Sentiment Analysis with Llama Models</center></h1>\")\n","    model_choice = gr.Dropdown(list(llama_models.keys()), label=\"Select Llama Model\")\n","    input_text = gr.Textbox(label=\"Enter text for sentiment analysis\", lines=4)\n","    output_text = gr.Textbox(label=\"Sentiment Result\")\n","    gr.Button(\"Analyze\").click(analyze_sentiment, [input_text, model_choice], output_text)"]},{"cell_type":"markdown","metadata":{},"source":["Launch the interface"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["demo.launch()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":2}
